General info
~~~~~~~~~~~~

17.11.2016]

The dead-line is December 5th. Although we should consider this date as fixed,
Daniels also said the presentation may be extended by few days (but definitely
within the week December 5-9).

We should focus only to Java apps (no C!) and we should only focus on web issues
(XSS and SQL injection) and nothing else. Moreover, the goal is to show real
errors, ideally no false positives. Definitelly nothing like Polyspace, or "the
other tool whose name is some number".

Daniel will provide us some Java packages to be evaluated for the presentation
(some Amazon's code). Our analysis must be fully automated: User simply uses
Maven to build his program (which possibly includes automati download and build
of dependent packages); Then he just runs our analyser on build binaries.

The demo will be LIVE in a sense, that the customer might want to see the
analysis running (he might be willing to wait for results). In any case
(i.e. even if he wants to see only outputs), then we have to provide HTML
output. This is important. Daniel said that they use always use outputs to HTML
(more precisely browser) and always.

They want to see in output from our analyser global statistics like histograms
and other forms of charts about what was analysed, how much time we spent where,
etc.

We will present the demo to Brian Cook (I hope I remembered the name correctly).
He is very skilled with program analysis (he worked on SLAM). He won't be
interested too much in shiny graphics output, but HTML is mandatory, and most
importantly, he is mainly interested in statistics, distributions of
anything/everything, graphs, etc.

Daniel said that no code clean up till the dead-line of that demo.

[18.11.2016]

We should analyse application who use Maven as a build system. So, Sakai is
actually one of apps we need. Also, the goal is to analyse as many such apps
as possible. 10, 100, ...

We are NOT allowed to do ANY modifications in those apps.

The goal of the demo is to show two things:
  1. FULL automation,
  2. We do better than tool Fortify
     (in terms of counts of produced reports per app)

It also means that we have to remove all modifications we did to Sakai so far
and analyse it in original untouched version.

We are not allowed to use tool Fortify ourselves for comparison (due to some
licensing issues). So, we have to make a list of Java app (build by Maven!)
and then we pass this list to Daniel, who will send it to some 3rd person who
will run Fortify on all apps in the list, and then we receive results (counts of
produced error reports per app) from Daniel. Numbers we produce should be
(must be) better than Fortify (i.e. lower the count better; but telling 0
everywhere would look suspicious).

[22.11.2016]

(a content of email from Daniel to Chris):

the priorities are, in order of decreasing importance:

1) Run on many programs.
2) Generate traces.
3) Be precise.

Priority one is an obvious one: we get data from runs on many programs that we
can use to inform further development; potential buyers get data that enables
buying decisions.
  
Priority two is motivated by the very clear steer we are getting from potential
customers. Even those where you'd expect a very safety-conscious approach
really do like traces/tests much more than lists of alarms.
  
Priority three, again, is motivated by what we hear from buyers. They much
rather miss a bug than wade through thousands of alarms. A cheap option to get
rid of false alarms will be to prioritize those alarms for which step (2) went
well. Precision of the abstract interpreter is then reduced to a performance
goal only, as it will prevent wasted search by (2).

[from conversation with Daniel, afternoon of the 22nd]

The user will supply us with the interface / method that is the (a) taint source
(b) taint sink, (c) sanitiser and (d) entry point. We should still automatically
fish these out of the JAR or whatever is generated by the build system.

The expectations above are long-term. For this demo we should produce some
concrete trace (does not need to be feasible) and ideally run on some medium
sized piece of software.

